{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "obj1= open(\"corpus.txt\",'r')\n",
    "in_corpus= (obj1.read())\n",
    "#print(in_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re, string, timeit\n",
    "from nltk.tokenize import sent_tokenize \n",
    "sentences = sent_tokenize(in_corpus)\n",
    "\n",
    "\n",
    "corpus= []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenizer= nltk.RegexpTokenizer(r\"[a-z’]+\")\n",
    "    newsentence= tokenizer.tokenize(sentence.lower()) #convert all to lower case\n",
    "    corpus.append(newsentence)\n",
    "    #print(newsentence)\n",
    "#replace words like _they_must to they must (and) why-should to why should\n",
    "multi_words = {\"-\":\" \", \"_\":\" \"}\n",
    "pattern = re.compile(\"|\".join(multi_words.keys()) + r'|_|-')\n",
    "\n",
    "#corpus = pattern.sub(lambda m: multi_words[re.escape(m.group(0))], str(corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua"
   },
   "outputs": [],
   "source": [
    "#Write code\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "uni_grams=[]\n",
    "bi_grams=[]\n",
    "tri_grams=[]\n",
    "four_grams=[]\n",
    "\n",
    "for content in corpus:\n",
    "    uni_grams.extend(content)\n",
    "    bi_grams.extend(ngrams(content,2))\n",
    "    tri_grams.extend(ngrams(content,3))\n",
    "    four_grams.extend(ngrams(content,4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n-grams without stop word removal\n",
      "\n",
      "Top 10 unigrams: \n",
      "the\n",
      "and\n",
      "to\n",
      "a\n",
      "she\n",
      "it\n",
      "of\n",
      "said\n",
      "i\n",
      "alice\n",
      "\n",
      "Top 10 bigrams: \n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('and', 'the')\n",
      "('in', 'the')\n",
      "('it', 'was')\n",
      "('to', 'the')\n",
      "('the', 'queen')\n",
      "('as', 'she')\n",
      "\n",
      "Top 10 trigrams: \n",
      "('the', 'mock', 'turtle')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('said', 'the', 'hatter')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'said', 'to')\n",
      "('she', 'went', 'on')\n",
      "\n",
      "Top 10 fourgrams: \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('will', 'you', 'won’t', 'you')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('in', 'a', 'great', 'hurry')\n",
      "('won’t', 'you', 'will', 'you')\n",
      "('well', 'as', 'she', 'could')\n",
      "('as', 'well', 'as', 'she')\n",
      "\n",
      "Top 10 n-grams with stop word removal\n",
      "\n",
      "Top 10 unigrams: \n",
      "said\n",
      "alice\n",
      "little\n",
      "one\n",
      "know\n",
      "like\n",
      "would\n",
      "went\n",
      "could\n",
      "thought\n",
      "\n",
      "Top 10 bigrams: \n",
      "('said', 'the')\n",
      "('said', 'alice')\n",
      "('the', 'queen')\n",
      "('the', 'king')\n",
      "('a', 'little')\n",
      "('mock', 'turtle')\n",
      "('the', 'mock')\n",
      "('the', 'gryphon')\n",
      "('the', 'hatter')\n",
      "('went', 'on')\n",
      "\n",
      "Top 10 trigrams: \n",
      "('the', 'mock', 'turtle')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('said', 'the', 'hatter')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'said', 'to')\n",
      "('she', 'went', 'on')\n",
      "\n",
      "Top 10 fourgrams: \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('will', 'you', 'won’t', 'you')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('in', 'a', 'great', 'hurry')\n",
      "('won’t', 'you', 'will', 'you')\n",
      "('well', 'as', 'she', 'could')\n",
      "('as', 'well', 'as', 'she')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words= list(set(stopwords.words('english')))\n",
    "\n",
    "\n",
    "print(\"Top 10 n-grams without stop word removal\")\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(uni_grams)\n",
    "print(\"Top 10 unigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "\n",
    "fdist = nltk.FreqDist(bi_grams)\n",
    "print(\"Top 10 bigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(tri_grams)\n",
    "print(\"Top 10 trigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(four_grams)\n",
    "print(\"Top 10 fourgrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top 10 n-grams with stop word removal\")\n",
    "print()\n",
    "\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in uni_grams if p not in stop_words]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "print(\"Top 10 unigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "\n",
    "bi_processed= []\n",
    "for gram in bi_grams:\n",
    "    cc=0\n",
    "    for word in gram:\n",
    "        if word in stop_words:\n",
    "            cc+=1\n",
    "    if cc!=2:\n",
    "        bi_processed.append(gram)\n",
    "\n",
    "\n",
    "fdist = nltk.FreqDist(bi_processed)\n",
    "print(\"Top 10 bigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "tri_processed= []\n",
    "for gram in tri_grams:\n",
    "    cc=0\n",
    "    for word in gram:\n",
    "        if word in stop_words:\n",
    "            cc+=1\n",
    "    if cc!=3:\n",
    "        tri_processed.append(gram)\n",
    "        \n",
    "#tri_processed = [gram for gram in trigrams if not all(stop in gram for stop in stop_words)]\n",
    "fdist = nltk.FreqDist(tri_processed)\n",
    "print(\"Top 10 trigrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "four_processed= []\n",
    "for gram in four_grams:\n",
    "    cc=0\n",
    "    for word in gram:\n",
    "        if word in stop_words:\n",
    "            cc+=1\n",
    "    if cc!=4:\n",
    "       # print(gram)\n",
    "        four_processed.append(gram)\n",
    "fdist = nltk.FreqDist(four_processed)\n",
    "print(\"Top 10 fourgrams: \")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    #print(u'{}:{}'.format(word, frequency))\n",
    "    print(word)\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 in each model after smoothing\n",
      "\n",
      "Bigram model:\n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('in', 'the')\n",
      "('it', 'was')\n",
      "('and', 'the')\n",
      "('at', 'the')\n",
      "('as', 'she')\n",
      "('to', 'the')\n",
      "\n",
      "\n",
      "Trigram model:\n",
      "('the', 'mock', 'turtle')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'hatter')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'mock')\n",
      "('she', 'went', 'on')\n",
      "('she', 'said', 'to')\n",
      "('said', 'the', 'caterpillar')\n",
      "\n",
      "\n",
      "Fourgram model:\n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('will', 'you', 'won’t', 'you')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('in', 'a', 'great', 'hurry')\n",
      "('won’t', 'you', 'will', 'you')\n",
      "('well', 'as', 'she', 'could')\n",
      "('the', 'moral', 'of', 'that')\n"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "n= len(set(uni_grams))\n",
    "unigramCounts= {}\n",
    "for gram in uni_grams:\n",
    "    if(gram in unigramCounts):\n",
    "        unigramCounts[gram]+=1\n",
    "    else:\n",
    "        unigramCounts[gram]=1\n",
    "model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "bigramCounts= {}\n",
    "for gram in bi_grams:\n",
    "    if(gram in bigramCounts):\n",
    "        bigramCounts[gram]+=1;\n",
    "    else:\n",
    "        bigramCounts[gram]=1\n",
    "    #print(gram,bigramCounts[gram])\n",
    "Prob_bi= {}\n",
    "\n",
    "for gram in (bi_grams):\n",
    "    Prob_bi[gram]= (bigramCounts.get(gram)+1)/(unigramCounts.get(gram[0])+n)\n",
    "    model2[gram[0]][gram[1]]= Prob_bi[gram]\n",
    "\n",
    "model3 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramCounts= {}\n",
    "for gram in tri_grams:\n",
    "    if(gram in trigramCounts):\n",
    "        trigramCounts[gram]+=1;\n",
    "    else:\n",
    "        trigramCounts[gram]=1\n",
    "    \n",
    "    #print(gram,bigramCounts[gram])\n",
    "Prob_tri= {}\n",
    "\n",
    "for gram in (tri_grams):\n",
    "    Prob_tri[gram]= (trigramCounts.get(gram)+1)/(bigramCounts.get((gram[0],gram[1]))+n)\n",
    "    #if(gram[0]==\"said\" and gram[1]==\"the\"):\n",
    "       # print(Prob_tri[gram],gram)\n",
    "    model3[(gram[0],gram[1])][gram[2]]= Prob_tri[gram]\n",
    "\n",
    "    \n",
    "model4 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "fourgramCounts= {}\n",
    "for gram in four_grams:\n",
    "    if(gram in fourgramCounts):\n",
    "        fourgramCounts[gram]+=1;\n",
    "    else:\n",
    "        fourgramCounts[gram]=1\n",
    "    if(gram[0]==\"alice\" and gram[1]== \"said\" and gram[2]== \"the\"):\n",
    "        print(gram,bigramCounts[gram])\n",
    "Prob_four= {}\n",
    "\n",
    "for gram in four_grams:\n",
    "    Prob_four[gram]= (fourgramCounts.get(gram)+1)/(trigramCounts.get((gram[0],gram[1],gram[2]))+n)\n",
    "    #print(Prob_four[gram],gram)\n",
    "    model4[(gram[0],gram[1],gram[2])][gram[3]]= Prob_four[gram]\n",
    "    \n",
    "print(\"Top 10 in each model after smoothing\")\n",
    "print()\n",
    "print(\"Bigram model:\")\n",
    "fdist = nltk.FreqDist(Prob_bi)\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "print()\n",
    "print(\"Trigram model:\")\n",
    "fdist = nltk.FreqDist(Prob_tri)\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n",
    "print()\n",
    "print()\n",
    "print(\"Fourgram model:\")\n",
    "fdist = nltk.FreqDist(Prob_four)\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Str1 Prediction\n",
      "\n",
      "Bigram model:\n",
      "after that alice said the queen\n",
      "after that alice said the king\n",
      "after that alice said the gryphon\n",
      "after that alice said the mock\n",
      "after that alice said the hatter\n",
      "\n",
      "\n",
      "Trigram model:\n",
      "after that alice said the king \n",
      "after that alice said the hatter \n",
      "after that alice said the mock \n",
      "after that alice said the caterpillar \n",
      "after that alice said the gryphon \n",
      "\n",
      "\n",
      "Fourgram model:\n",
      "NULL\n",
      "NULL\n",
      "NULL\n",
      "NULL\n",
      "NULL\n",
      "\n",
      "\n",
      "Str2 Prediction\n",
      "\n",
      "Bigram model:\n",
      "alice felt so desperate that she was a \n",
      "alice felt so desperate that she was the \n",
      "alice felt so desperate that she was not \n",
      "alice felt so desperate that she was going \n",
      "alice felt so desperate that she was that \n",
      "\n",
      "\n",
      "Trigram model:\n",
      "alice felt so desperate that she was now \n",
      "alice felt so desperate that she was quite \n",
      "alice felt so desperate that she was a \n",
      "alice felt so desperate that she was walking \n",
      "alice felt so desperate that she was looking \n",
      "\n",
      "\n",
      "Fourgram model:\n",
      "alice felt so desperate that she was now \n",
      "alice felt so desperate that she was losing \n",
      "alice felt so desperate that she was ready \n",
      "alice felt so desperate that she was walking \n",
      "alice felt so desperate that she was in \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list1 = str1.split()\n",
    "print(\"Str1 Prediction\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Bigram model:\")\n",
    "cc=0\n",
    "fdist = nltk.FreqDist(model2[(word_list1[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    cc+=1\n",
    "    #print(word)\n",
    "    #break\n",
    "    if(model2[(word_list1[-1])][word]==0):\n",
    "        print(\"NULL\")\n",
    "    else:   \n",
    "        print(str1+\" \"+word)#,model2[(word_list1[-1])][word])\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Trigram model:\")\n",
    "cc=0\n",
    "fdist = nltk.FreqDist(model3[(word_list1[-2],word_list1[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    cc+=1\n",
    "    #print(word)\n",
    "    if(model3[(word_list1[-2],word_list1[-1])][word]==0):\n",
    "        print(\"NULL\")\n",
    "    else:\n",
    "        print(str1+\" \"+word+\" \")#,model3[(word_list1[-2],word_list1[-1])][word])\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n",
    "\n",
    "\n",
    "cc=0\n",
    "print(\"Fourgram model:\")\n",
    "fdist = nltk.FreqDist(model4[(word_list1[-3],word_list1[-2],word_list1[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    cc+=1\n",
    "    print(str1+\" \"+word+ \" \")#,model4[(word_list1[-3],word_list1[-2],word_list1[-1])][word])\n",
    "\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_list2 = str2.split()\n",
    "print(\"Str2 Prediction\")\n",
    "print()\n",
    "print(\"Bigram model:\")\n",
    "cc=0\n",
    "fdist = nltk.FreqDist(model2[(word_list2[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    cc+=1\n",
    "    print(str2+\" \"+word+\" \")#,model2[(word_list2[-1])][word])\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Trigram model:\")\n",
    "cc=0\n",
    "fdist = nltk.FreqDist(model3[(word_list2[-2],word_list2[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    #print(word)\n",
    "    cc+=1\n",
    "    print(str2+\" \"+word+\" \")#,model3[(word_list2[-2],word_list2[-1])][word])\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fourgram model:\")\n",
    "cc=0\n",
    "fdist = nltk.FreqDist(model4[(word_list2[-3],word_list2[-2],word_list2[-1])])\n",
    "for word, frequency in fdist.most_common(5):\n",
    "    #print(word)\n",
    "    cc+=1\n",
    "    print(str2+\" \"+word+\" \")#,model4[(word_list2[-3],word_list2[-2],word_list2[-1])][word])\n",
    "print(\"NULL\\n\"*(5-cc))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
